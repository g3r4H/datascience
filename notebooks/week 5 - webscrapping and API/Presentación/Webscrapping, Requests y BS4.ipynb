{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 3\n",
    "Durante las semanas previas aprendieron a usar varias herramientas de Python; desde Python vanilla (sin el uso de paquetes) hasta el uso de los paquetes más improtantes para el análisis numérico en Python (Numpy, Matplotlib y Pandas). Con estas herramientas se puede realizar gran parte del análisis estadístico de datos; sin embargo, esta semana no lidiara con el análisis de datos; de hecho se podría decir que estamos dando un paso atrás en el proceso de Data Science.\n",
    "\n",
    "<img src=\"img/dataprog.png\"/>\n",
    "\n",
    "Nosotros nos vamos a enfocar en el área de Data Gathering; el hecho de que el diagrama no haga énfasis en esta parte del proceso no le resta su importancia ya que si no tenemos buenos datos no podemos tener buenas conclusiones. En esta semana vamos a seguir el proceso de obtención y pre-procesamiento de datos: Primero veremos Data Modeling para ver las formas en las cuales los datos se relacionan y se guardan; después veremos como podemos obtener datos de internet de una forma eficiente y finalmente haremos actividades para poner a prueba los conocimientos de esta semana.\n",
    "\n",
    "# Objetivos\n",
    "- Conocer un poco acerca del proceso de obtención y almacenamiento de datos.\n",
    "- Ser capaces navegar los tags en un sitio de internet.\n",
    "- Aprenda a utilizar la librería requests y beautifulsoup4 para obtener información de estos sitios.\n",
    "- Conozca lo que es una API y sea capaz de utilizar API's básicas.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como podemos obtener datos?\n",
    "Los datos en general se pueden obtener de muchas formas; puede ser a través de dispositivos electrónicos (sensores) o de una interacción más humana (encuestas telefónicas o en línea), la verdad es que las formas de obtener datos son tan diversas como los datos mismos y estas suelen cambiar sin embargo, dependiendo de la situación en la cual nos encontremos vamos a necesitar diferentes tipos de datos. En el ámbito empresarial nos interesarían datos internos de nuestra empresa como lo son datos de empleados, desempeño de programas, indicadores financieros, días de inventario, entre otros; también nos pueden interesar datos de entes externos como proveedores, almacenistas, transportistas y clientes. Ya que reducimos la variedad de datos ahora podemos recolectarlos y eventualmente almacenarlos; sin embargo, este no es un proceso de caja negra. Este proceso necesita de conocimiento previo de los datos, las entidades involucradas y conocer los esquemas de almacenamiento para escoger al que vaya a ser de más ayuda en nuestro caso. Para esto se usan modelos de datos y vamos a ver las bases suficientes para planear de forma correcta el almacenamiento de datos.\n",
    "\n",
    "## Data modeling\n",
    "Un modelo de datos es una representación (generalmente gráfica) en la cual se pueden observar los contenidos, relaciones y jerarquías de los elementos que conforman una base de datos. Es un esqueleto con las reglas que van a regir nuestra base de datos; por lo tanto, debe de ser rígido pero lo suficientemente robusto para evitar errores; por lo tanto, hay que seguir un proceso cuando vayamos a querer crear una base de datos para asegurarnos de que la base de datos esté orientada a las necesidades de la empresa. El proceso más seguido para la creación de una base de datos es seguir el orden de (Conceptual, Logical y Physical) el cual nos permite organizar nuestros datos casi independientemente del software que utilizemos para guardar los datos. \n",
    "\n",
    "<img src=\"img/Datamod.png\"/>\n",
    "\n",
    "### Modelo Conceptual\n",
    "\n",
    "El modelo conceptual se hace para explicar la relación de las entidades (personas, empresas, almacenes, etc.) a gente fuera del área de programación; es decir, se observa más al comportamiento de las entidades que la información numérica contenida. En el ejemplo anterior estamos viendo una base de datos sencilla en donde tenemos 3 entidades (Estudiantes, Cursos y cursos inscritos). En este diagrama apreciamos que la entidad (cursos inscritos) necesita del estudiante y de la lista global de cursos; esto nos dice que a un estudiante se le asignan ciertos cursos inscritos.\n",
    "\n",
    "La ventaja de usar inicialmente un modelo conceptual es que todavía no pensamos en la información; cuando nuestro enfoque principal son los individuos y no los datos esto nos permite mostrar el sistema de la forma más sencilla y general. Una vez que se tengan todas las entidades entonces ya se pueden trabajar sobre ellas e ir agregando datos lo cual llevará al siguiente modelo.\n",
    "\n",
    "### Modelo Logico\n",
    "\n",
    "Regresando a la imágen; podemos ver que el modelo lógico ocupa más espacio que el conceptual y con mucha razón. Un modelo lógico todavía no considera que herramienta vamos a usar para guardar los datos; sin embargo, ahora ya nos enfocamos en los datos contenidos en cada entidad y las relaciones entre ellos. Regresando al caso de la universidad; ahora vemos que el estudiante cuenta con varios atributos que lo identifican como su matrícula, nombre y correo. También podemos ver que el curso tiene sus propiedades como ID, nombre, instructor y horario, es por medio de la combinación de ambos datos que podemos tener una inscripción a la cual se le asocia un estudiante y cursos; sin embargo, la inscripción tiene sus propios datos como calificaciones y estatus. Cuando antes solo veiamos una relación simple entre las entidades; ahora podemos ver que elementos conforman esa relación.\n",
    "\n",
    "Este modelo sigue siendo independiente del sistema de administración de datos; por lo cual por ahora no nos concentramos mucho en como van a estar almacenados ni en el tipo de variable. Una vez que tengamos establecido el modelo lógico ya podemos pasar a la implementación final.\n",
    "\n",
    "### Modelo Físico\n",
    "\n",
    "El modelo físico es lo más similar que vamos a tener a la base de datos; en lugar de solamente tener los elementos de la relación estos son desmenuzados hasta sus componentes fundamentales. Por ejemplo: Si volvemos a ver la casilla de estudiante podemos ver que Address se ha dividido en (Address1, Address2, postal code, city y state) que son partes del elemento Address. No solo se han encontrado las componentes fundamentales; sino también se indica de que tipo va a ser la variable y en algunos casos la longitud máxima que puede tener (esto ya termina afectando el sistema de administración de de datos). Este modelo físico es el que ya se terminaría implementando para establecer las reglas de todos los futuros datos que van a entrar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs NoSQL (Relacional vs Documento)\n",
    "Una vez que hemos pasado por el proceso de crear el modelo físico, es hora de seleccionar un sistema de base de datos. Hay que estar muy seguros de que conozcamos bien las relaciones encontradas en el modelo físico ya que de ello depende el tipo de sistema que vamos a usar. Los dos sistemas dominantes son SQL (Structured Query Language) y NoSQL (Not only SQL); el modelo SQL se basa en el uso de tablas altamente estructuradas y ordenadas que se asemejan bastante a un excel o como vemos los Pandas en Python. Tenemos columnas que representan elementos o datos y renglones que representan observaciones; por lo tanto, todas las observaciones tienen el mismo número de elementos asociados a ellos.\n",
    "\n",
    "<img src=\"img/SQL.png\"/>\n",
    "\n",
    "\n",
    "Un ejemplo de datos que se puede implementar facilmente en SQL es un registro de transacciones; cada transacción tiene un emisor, un receptor, una cantidad y una fecha. Debido a que todos los elementos son muy similares entonces se pueden usar una o más tablas para representar toda la información. Por lo tanto; la ventaja de SQL es que la estructura rígida puede facilitar mucho la accessibilidad de los datos; asimismo, es un sistema que lleva vigente desde los 60's sin ser reemplazado por lo cual la comunidad y el soporte están muy bien fundamentados; sin embargo, también cuenta con sus desventajas. Muchos programas que usan SQL no son open-source y estos sistemas no soportan bien un gran número de read-write requests. Si uno quisiera incrementar la capacidad de esto se necesitaria aumentar las especificaciones de 1 solo servidor por lo cual hay una limitante tecnológica a esto.\n",
    "\n",
    "Por otra parte; NoSQL renuncia a la estructura por medio de tablas por lo cual también renuncia a varios de los beneficios de SQL; sin embargo, mucha información se encuentra guardad en este formato debido a que es fácil de procesar multiples read-write requests. Los sensores e información proveniente de (Big Data) se maneja en este formato por lo cual su relevancia no puede ser ignorada. NoSQL sigue un formato libre (en este caso nos enfocamos en el modelo de documento) lo cual nos permite almacenar la información dentro de documentos json o en un formato anidado. ¿Que otra ventaja nos da? Para ello hay que ver relaciones lo cual suena extraño ya que esto no es una base de datos relacional. \n",
    "\n",
    "### Relaciones\n",
    "Un elemento puede tener varios tipos de relaciones (many to many), (one to many) o (many to one). Por ejemplo; supongamos que tenemos una base de datos en donde se guarda la información de aplicantes a una empresa. Ahí tenemos valores únicos como nombre, correo y dirección; sin embargo, no todos cuentan con el mismo número de trabajos previos. El campo de trabajos tiene una relación one to many ya que es muy poco probable que diferentes personas tengan la misma experiencia en las mismas empresas; asimismo, varios aplicantes pueden ir a diferentes escuelas pero en general estas se repiten entre personas (many to many). En el caso one to many y many to many es cuando se recomendaría utilizar NoSQL sobre SQL; sin embargo, esto no significa que los datos no pueden ser representados usando SQL solo que su almacenamiento se puede complicar y ser ineficiente comparado con las soluciones NoSQL.\n",
    "\n",
    "### ¿Que nos podemos llevar de esto?\n",
    "Lo que podemos ver es que antes de pensar en conseguir y utilizar información se tiene que saber bajo que esquema esta se va a guardar. Las conexiones, relaciones y forma de almacenamiento de los datos son cuestiones que pueden terminar haciendo más difícil o imposible el uso de esos datos para proyectos interesantes. Por lo tanto; tenemos que desarrollar buenas prácticas para que nuestros datos tengan una estructura que esté de acuerdo a su naturaleza.\n",
    "\n",
    "## Webscrapping\n",
    "La definición de Webscrapping es la obtención y preprocesamiento de datos que se originan de internet; ahora, aunque tengamos acceso a mucha información por medio de preguntas, encuestas y bases de datos locales de la empresa; no podemos negar que la mayor fuente de información del mundo es el internet. Desde Amazon para un análisis del posicionamiento digital de la competencia, hasta la INEGI para realizar un estudio demográfico o obtener indicadores macroeconómicos; en la actualidad, casi todo se puede encontrar de una forma u otra en línea y muchas empresas e individuos están buscando aprovechar esto para hacer proyectos interesantes.\n",
    "\n",
    "### ¿Como se estructuran los elementos de una página web?\n",
    "Aunque la apariencia de una página web puede parecer ordenada al usuario; al entrar y ver el código se puede comenzar a ver que tan complicado puede llegar a ser el scrapping. Hay muchos elementos con diferentes jerarquías; elementos que contienen elementos dentro de otros elementos. Cuando hacemos Webscrapping en realidad estámos recuperando el código html de la página web por lo que es importante que sepamos navegarlo para recuperar la información que nos interesa. Este proceso no es fácil pero utilizando Python este se puede hacer un poco menos complicado.\n",
    "<img src=\"img/html.png\"/>\n",
    "<img src=\"img/dom-tree.png\" />\n",
    "\n",
    "Al contar con una estructura anidada; podemos ver que las páginas web no son relacionales debido a que cuentan con diferentes niveles y jerarquías para sus elementos. Aunque la representación relacional es posible; la no relacional termina siendo la más intuitiva en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSA del uso de la información\n",
    "Reuerden que sin importar de donde estemos adquiriendo la información, detrás de esos números se encuentran seres humanos como nosotros por lo cual es importante no solo cuidarla sino darle un uso apropiado. Actualmente no hay leyes tan fuertes en cuanto al uso y venta de información; sin embargo, solo por que la ley no lo prohibe explicitamente no significa que podemos hacer lo que queramos con la información. Ya hay casos muy sonados de recollección y venta de información de grandes empresas como Facebook y Google; sin embargo, es muy difícil que algo de ese nivel pase en estas empresas con algunas precauciones. Avisos de privacidad, transparencia, hacer a los usuarios anónimos y rendir cuentas a los que comparten la información.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora a programar!\n",
    "\n",
    "## Requests\n",
    "Cuando queremos acceder a una página de internet nosotros tecleamos la dirección en el navegador; después, nuestro navegador hace una petición al servidor para recibir la información acerca de la página a la que queremos acceder. Un request es una petición para entrar a la página de nuestro interés; ahora que se hace un request a la página se debe de decidir el método que se va a realizar.\n",
    "\n",
    "Lo primero que podemos hacer es obtener la información, esto se hace con el get. Es una solicitud para recuperar el código de la página de internet. Esté método se puede utilizar en python importando el paquete de requests; al correr el método dir sobre el objeto request podemos obtener una lista que nos dice la información y las etiquetas de la página que podemos acceder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get('http://www.amazon.com')\n",
    "dir(page)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, recuperamos un objeto muy grande que puede tener mucha información pero esta no se encuentra tan a la vista como lo puede ser en un documento word. Si queremos acceder a esta información tenemos que utilizar la notación de Python de .objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.url, page.request, page.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque si vemos la página, podemos ver que no tenemos los permisos apropiados para scrapear amazon y nos regresa un mensaje automatizado para poder scrapearla con su permiso. Muchos sitios web tienen en pie este tipo de restricciones por lo que es importante investigar antes de tiempo cuales sitios soportan el scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es publicar información; ya sea crearla o actualizarla. Esto se hace por medio del método post (sigue siendo un request); con esta se manda una petición a un sitio web de práctica para agregar información al segmento de form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://httpbin.org/post', \n",
    "                  data = {'Nombre':'Yo'})\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresando al método get; nosotros podemos utilizar ciertas funciones para manipular de forma específica a donde queremos entrar de la página. En ciertos casos esto se usa para accedar a funciones específicas dentro de una API; por lo tanto, se establece una llave y un valor el cual se incluye dentro de la dirección web de la siguiente forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', \n",
    "           'key2': ['value2', 'value3']}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos mandar solicitudes para borrar información o solamente probar la respuesta del servidor; estos son los métodos delete y head. Para scrapping no los usaremos mucho ya que en si no queremos modificar la información del documento sino queremos extraerla. Aunque en caso de querer hacerlo debemos de contar con los permisos correspondientes ya que la misma página tiene que poner restricciones para evitar que cualquier persona modifique la página a voluntad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup4\n",
    "Ahora; los nombres de los paquetes de Python muchas veces se basan en obras de la cultura popular. Esto surge de Alicia en el país de las maravillas debido a las personas que hacen el programa se reservan los derechos de nombrarlo. Ahora bien; esta libreria es muy similar a la de requests ya que podemos hacer los requests tradicionales; pero la diferencia está en que con este paquete ustedes pueden navegar la estructura de la pagina web con mucha mayor facilidad que utilizando Python vanilla. Esto nos permite extraer la información de la página web y utilizarla para nuestras propias cuestiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.milenio.com'\n",
    "URL = BeautifulSoup(requests.get(URL).text, \"lxml\")\n",
    "dir(URL)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este objeto no es un código html, es lo que se le conoce como una sopa (tiene muchos ingredientes pero los tenemos bien catalogados). Con estos comandos podemos reobtener parámetros interesantes como con el método de requests pero en este caso podemos explorar muchas cosas más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(URL.title, URL.title.name, URL.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de find_all nos permite buscar el tag del renglón incluyendo el texto y las demás propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL.find_all('a')[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo se pueden extraer los subenlaces que llevan a diferentes regiónes de la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [a['href'] for a in URL.find_all('a', href=True)]\n",
    "print(ref[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el renglón cuenta con una etiqueta y una clase, esta se puede especificar dentro del método junto con atributos como href. En el siguiente ejemplo accedemos a la etiqueta li y la clase father para extraer el primer elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = URL.find_all('li', \n",
    "                     class_='father')\n",
    "print(lista[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta lista podemos extraer el texto con el método get_text(). Usamos el método .strip() para quitar los espacios y tabs adicionales que puede tener el formato del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos los resultados\n",
    "Categorias=[str(lista[i].get_text()).strip() \n",
    "            for i in range(0,len(lista)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo; esto no nos quita las restricciones que nos ponen las página de internet por lo cual si intentamos importar walmart puede que el objeto cuente con diferentes categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1 = 'https://www.walmart.com.mx/computadoras/laptops'\n",
    "URL1 = BeautifulSoup(requests.get(URL1).text, \"lxml\")\n",
    "dir(URL1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero al usar el método index... vemos que nuestro servidor no puede acceder a la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio!\n",
    "Entra a un departamento de tu preferencia en la página de Soriana (Repostería, Celulares, etc). Obten los siguientes datos de un producto.\n",
    "\n",
    "-  Nombre del Producto\n",
    "-  Precio actual\n",
    "\n",
    "Una vez que lo tengas, encuentra una forma de extraer esos datos para los primeros 3 productos usando un ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First product\n",
      "----------------\n",
      "Ideapad Lenovo 110 Intel Celeron N3060  4 GB RAM 500 GB...\n",
      "$8,999.00\n",
      "\n",
      "First 3 products\n",
      "----------------\n",
      "Ideapad Lenovo 110 Intel Celeron N3060  4 GB RAM 500 GB...\n",
      "$8,999.00\n",
      "\n",
      "Laptop HP 14-CM0021LA AMD 7ma Gen 4 GB RAM 500 GB ROM 1...\n",
      "$6,499.00\n",
      "\n",
      "Laptop HP 15-DA0009LA Intel Ci3 7ma Gen 8 GB RAM 1 TB R...\n",
      "$9,999.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#URL2 = 'https://www.soriana.com/soriana/es/c/Hogar/Cocina/Reposteria-y-gourmet/c/36C'\n",
    "#URL2 = BeautifulSoup(requests.get(URL2).text, \"lxml\")\n",
    "\n",
    "url_laptops = 'https://www.soriana.com/soriana/es/c/Computo/Equipos-de-Computo/Laptops/c/211'\n",
    "page = requests.get(url_laptops)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "products_grid = soup.find('div', class_='product__listing product__grid')\n",
    "products = products_grid.find_all('div', class_='product-item')\n",
    "\n",
    "# Get first product\n",
    "first_prodcut_name = products[0].find('a', class_='name').getText()\n",
    "first_prodcut_price = products[0].find('span', class_='price').getText()\n",
    "print('First product')\n",
    "print('----------------')\n",
    "print(first_prodcut_name.strip())\n",
    "print(first_prodcut_price.strip())\n",
    "print('')\n",
    "\n",
    "# Get first 3 products\n",
    "print('First 3 products')\n",
    "print('----------------')\n",
    "products_list = []\n",
    "for num in range(3):\n",
    "    product_soup = products[num]\n",
    "    product_name = product_soup.find('a', class_='name').getText()\n",
    "    product_price = product_soup.find('span', class_='price').getText()\n",
    "    product = {}\n",
    "    product['name'] = product_name.strip()\n",
    "    product['price'] = product_price.strip()\n",
    "    products_list.append(product)\n",
    "\n",
    "for product in products_list:\n",
    "    print(product['name'])\n",
    "    print(product['price'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos hacer un dataframe y transformar una base de datos no relacional a una relacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataframe\n",
      "----------------\n",
      "                                             Product       Price\n",
      "0  Ideapad Lenovo 110 Intel Celeron N3060  4 GB R...   $8,999.00\n",
      "1  Laptop HP 14-CM0021LA AMD 7ma Gen 4 GB RAM 500...   $6,499.00\n",
      "2  Laptop HP 15-DA0009LA Intel Ci3 7ma Gen 8 GB R...   $9,999.00\n",
      "3  Laptop Lenovo Legion Y530 Intel Ci5 8va Gen 8 ...  $22,999.00\n",
      "4  Laptop HP 15.6 plg Windows 10 Home 4 GB RAM y ...   $9,999.00\n"
     ]
    }
   ],
   "source": [
    "print('Create Dataframe')\n",
    "print('----------------')\n",
    "names = []\n",
    "prices = []\n",
    "for product in products:\n",
    "    names.append(product.find('a', class_='name').getText().strip())\n",
    "    prices.append(product.find('span', class_='price').getText().strip())\n",
    "\n",
    "df = pd.DataFrame({'Product': names, 'Price': prices})\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
